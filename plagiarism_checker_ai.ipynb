{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6af5696-e55e-4167-9b5f-bb550fc72860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\surbh\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e53c4de-9fb5-40a4-85ca-a9e6c47b14e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d102eab1-25b4-418b-b323-4f2c601ebf20",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2724430288.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfile_path = 'C:\\Users\\surbh'\u001b[39m\n                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load data from train_snli.txt\n",
    "\n",
    "file_path = 'C:\\Users\\surbh'\n",
    "\n",
    "# Read the file line by line\n",
    "lines = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(\"Sample line:\", lines[0])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6acff75e-d6dd-407e-9657-9d3ecf6cad43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\surbh\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67e120b5-1aeb-4037-99b4-166d7deadb2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\surbh\\\\mit-plagiarism-detection-dataset\\\\train_snli.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Read the file line by line\u001b[39;00m\n\u001b[32m      4\u001b[39m lines = []\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m  f:\n\u001b[32m      6\u001b[39m     lines = f.readlines()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal lines: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(lines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:326\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    321\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    324\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\surbh\\\\mit-plagiarism-detection-dataset\\\\train_snli.txt'"
     ]
    }
   ],
   "source": [
    "file_path = 'C:\\\\Users\\\\surbh\\\\mit-plagiarism-detection-dataset\\\\train_snli.txt'\n",
    "\n",
    "# Read the file line by line\n",
    "lines = []\n",
    "with open(file_path, 'r', encoding='utf-8') as  f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(\"Sample line:\", lines[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "864a3986-35e9-4817-9115-a1dc3767ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:\\\\Users\\\\surbh\\\\mit-plagiarism-detection-dataset\\\\train_snli.txt' \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1de05cd-a84c-483d-873f-9f7b943b15c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = 'C:\\\\Users\\\\surbh\\\\mit-plagiarism-detection-dataset\\\\train_snli.txt'\n",
    "\n",
    "print(\"File exists:\", os.path.exists(file_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71220ae5-4a11-4f86-a894-f8dbb39d2fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "file_path = 'mit-plagiarism-detection-dataset/train_snli.txt'\n",
    "\n",
    "import os\n",
    "print(\"File exists:\", os.path.exists(file_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95f4b65b-7bee-41ad-97e7-4dab8c5057d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 367373\n",
      "Line 1: A person on a horse jumps over a broken down airplane.\tA person is at a diner, ordering an omelette.\t0\n",
      "\n",
      "Line 2: A person on a horse jumps over a broken down airplane.\tA person is outdoors, on a horse.\t1\n",
      "\n",
      "Line 3: Children smiling and waving at camera\tThere are children present\t1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the file line by line\n",
    "lines = []\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "\n",
    "# Print first 3 sample lines\n",
    "for i in range(3):\n",
    "    print(f\"Line {i+1}:\", lines[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca5e155e-5e5e-4dd1-90db-33cd106e1144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            original  \\\n",
      "0                          A track event held by J.P   \n",
      "1                          A track event held by J.P   \n",
      "2  A classroom of students discussing lecture.\\tT...   \n",
      "3  A group of people marching in a parade waving ...   \n",
      "4  An urban, outdoor, daytime setting shows a bui...   \n",
      "\n",
      "                                             suspect  label  \n",
      "0                   Morgan Chase with security.\\tJ.P      1  \n",
      "1  Morgan Chase with security.\\tThe track event h...      1  \n",
      "2                                          class.\\t0      1  \n",
      "3                                They are outside\\t1      1  \n",
      "4  1 ALLEN ST.\".\\tA set of pallets are being runo...      1  \n",
      "Total samples: 537\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Process lines into sentence pairs\n",
    "sentence1 = []\n",
    "sentence2 = []\n",
    "\n",
    "for line in lines:\n",
    "    parts = line.strip().split('. ')\n",
    "    if len(parts) >= 2:\n",
    "        sentence1.append(parts[0].strip())\n",
    "        sentence2.append(parts[1].strip())\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'original': sentence1,\n",
    "    'suspect': sentence2\n",
    "})\n",
    "\n",
    "# Temporary: Add dummy labels (1 for all, real label not provided in file)\n",
    "df['label'] = 1\n",
    "\n",
    "print(df.head())\n",
    "print(\"Total samples:\", len(df)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfcb4eef-eacd-44c2-ace2-935b90cb1221",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     20\u001b[39m model = LogisticRegression()\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Predict & evaluate\u001b[39;00m\n\u001b[32m     24\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:1301\u001b[39m, in \u001b[36mLogisticRegression.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1299\u001b[39m classes_ = \u001b[38;5;28mself\u001b[39m.classes_\n\u001b[32m   1300\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1301\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis solver needs samples of at least 2 classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1303\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m in the data, but the data contains only one\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1304\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m % classes_[\u001b[32m0\u001b[39m]\n\u001b[32m   1305\u001b[39m     )\n\u001b[32m   1307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.classes_) == \u001b[32m2\u001b[39m:\n\u001b[32m   1308\u001b[39m     n_classes = \u001b[32m1\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: np.int64(1)"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Combine both texts into one feature\n",
    "df['combined'] = df['original'] + \" \" + df['suspect']\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['combined'])\n",
    "\n",
    "# Labels (still dummy 1s, just for demo)\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d911264b-cb95-4a26-ab8c-3d9b6fba9a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "1    276\n",
      "0    261\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Randomly assign 0 or 1 as labels for demo purpose\n",
    "np.random.seed(42)\n",
    "df['label'] = np.random.randint(0, 2, size=len(df))\n",
    "\n",
    "print(df['label'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14640763-6f9a-44a4-b22c-478958cfb435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5277777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Combine both texts into one feature\n",
    "df['combined'] = df['original'] + \" \" + df['suspect']\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(df['combined'])\n",
    "\n",
    "# Labels\n",
    "y = df['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict & evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "999c6cce-fd41-4379-b682-d88f920a1cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Plagiarized\n"
     ]
    }
   ],
   "source": [
    "def predict_plagiarism(original_text, suspect_text):\n",
    "    combined_input = original_text + \" \" + suspect_text\n",
    "    vectorized_input = vectorizer.transform([combined_input])\n",
    "    prediction = model.predict(vectorized_input)[0]\n",
    "    result = \"Plagiarized\" if prediction == 1 else \"Not Plagiarized\"\n",
    "    print(\"Prediction:\", result)\n",
    "\n",
    "# Example test\n",
    "original = \"A man is riding a bike on the street.\"\n",
    "suspect = \"A person rides a bicycle down the road.\"\n",
    "\n",
    "predict_plagiarism(original, suspect) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e111c3a-3950-4c87-a2a3-aa9b60fe3a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
